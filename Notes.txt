Task1:
Only looked at individual variable performance

Task 2:
Columns Removed:
	- Remove all NaN columns
	- Removed columns with cardinality = 1
	- Removed feature is highly correlated (acc>0.8) with target
	- Removed columns which can be irrelevant like names, ids etc
Preprocessing: 
	- Only one hot encoding

Finally - 6 (5  cat, 1 float) 

##############################
For some reason, the payments
variable was getting treated 
as an object variable. So 
explicitly converted that into
float
##############################

Task 3:
a)
Columns Added:
	- Columns with fewer than 50% nulls
Imputation:
	- Constant
Columns Removed:
	- Columns with more than 50% nulls
	- Removed columns which can be irrelevant like names, ids etc
	- Having >= 100 categories
Preprocessing:
	- OHE
	- Scaling

b) 
a) + Imputation = Mode	

########################################
 most_frequent better than Constant
Using strategy ='most_frequent' going ahead
for low cardinality categoricals
########################################

c)
b) + Preprocessing
		- Target Encoding (imputation = constant)

Task 4:
